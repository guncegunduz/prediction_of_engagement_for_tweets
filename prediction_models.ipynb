{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53beb03",
   "metadata": {
    "id": "e53beb03"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5hSsirXY3HsF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hSsirXY3HsF",
    "outputId": "78375973-e5e6-490d-e7cf-9f0b30cd0e90"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Slh7tq6QYpps",
   "metadata": {
    "id": "Slh7tq6QYpps"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/gdrive/MyDrive/data/final_no_edit_needed.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d5e2c",
   "metadata": {
    "id": "804d5e2c"
   },
   "outputs": [],
   "source": [
    "#outlıer elımınatıon\n",
    "df = df[(df['Engagement_Total']<1000)]\n",
    "df = df.groupby('source').filter(lambda x : len(x)>10)\n",
    "df = df.reset_index(drop=True)\n",
    "words = [\"#\"]\n",
    "df[\"check_hashtags\"] = df.text.apply(lambda x: sum([x.count(word) for word in words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZEyVxXy6vau4",
   "metadata": {
    "id": "ZEyVxXy6vau4"
   },
   "outputs": [],
   "source": [
    "!pip install textstat\n",
    "import textstat\n",
    "def readingscore(text):\n",
    "\n",
    "    score = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    return score\n",
    "\n",
    "df['flesch_reading'] = df['text_preprocessed'].apply(readingscore)\n",
    "\n",
    "#outlıer elımınatıon\n",
    "df = df[(df['flesch_reading']>-900)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DjUETBsCH1D1",
   "metadata": {
    "id": "DjUETBsCH1D1"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlim(-75,150)\n",
    "plt.xlabel('Flesch Reading Score')\n",
    "plt.ylabel('Density')\n",
    "sns.kdeplot(df['flesch_reading'],shade=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ggf_wsiZsTNr",
   "metadata": {
    "id": "ggf_wsiZsTNr"
   },
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceSFqf1Srj45",
   "metadata": {
    "id": "ceSFqf1Srj45"
   },
   "outputs": [],
   "source": [
    "#df.sort_values(by='flesch_reading', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828119f",
   "metadata": {
    "id": "7828119f"
   },
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91wDJ31R44zN",
   "metadata": {
    "id": "91wDJ31R44zN"
   },
   "outputs": [],
   "source": [
    "# checking if there are any duplicates by id column\n",
    "duplicateRowsDF = df[df.duplicated(['id'])]\n",
    "duplicateRowsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1ba8e",
   "metadata": {
    "id": "c9c1ba8e"
   },
   "source": [
    "# Profanity Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec1c76",
   "metadata": {
    "id": "ecec1c76"
   },
   "outputs": [],
   "source": [
    "!pip install alt-profanity-check\n",
    "from profanity_check import predict, predict_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c33aef6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c33aef6",
    "outputId": "b8dda495-0633-4330-b0fe-8d3a6cc05995"
   },
   "outputs": [],
   "source": [
    "df[\"profanity_probability\"] = np.nan\n",
    "for i in range(df.shape[0]):\n",
    "    df['profanity_probability'][i] = str(predict_prob([df['text_preprocessed'][i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546b03d",
   "metadata": {
    "id": "4546b03d"
   },
   "outputs": [],
   "source": [
    "df['profanity_probability'] = df['profanity_probability'].map(lambda x: x.lstrip('['))\n",
    "df['profanity_probability'] = df['profanity_probability'].map(lambda x: x.rstrip(']'))\n",
    "df['profanity_probability'] = df['profanity_probability'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0b00c",
   "metadata": {
    "id": "e6b0b00c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e868691",
   "metadata": {
    "id": "8e868691"
   },
   "source": [
    "# Classifying Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28616a02",
   "metadata": {
    "id": "28616a02"
   },
   "outputs": [],
   "source": [
    "df = df.loc[:, df.columns.isin([\"id\",\"author_id\",\n",
    "                           \"created_at\",\"text\",\"source\",\"public_metrics.like_count\",\n",
    "                           \"public_metrics.quote_count\",\"public_metrics.reply_count\",\"public_metrics.retweet_count\",\"possibly_sensitive\",\n",
    "                           \"author.id\",\"author.created_at\",\"author.public_metrics.followers_count\",\n",
    "                           \"author.public_metrics.listed_count\",\"author.public_metrics.tweet_count\",\n",
    "                           'time_diff_hours','Engagement_Total','text_preprocessed','avg_length_words',\n",
    "                            'emoji_count','text_preprocessed_nostopwords','created_at_hour','check_hashtags',\n",
    "                            'check_media','check_poll', 'day','day_name',\n",
    "                            'url_count','mention_count','text_preprocessed_emotion_scores','text_preprocessed_sentiment','flesch_reading','text_preprocessed_emotion_heading', 'profanity_probability'])]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edef827",
   "metadata": {
    "id": "5edef827"
   },
   "outputs": [],
   "source": [
    "#Deciding on class ranges\n",
    "df[(df['Engagement_Total']>0)][['Engagement_Total']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46e8a5",
   "metadata": {
    "id": "3c46e8a5"
   },
   "outputs": [],
   "source": [
    "def engagement_class(x):\n",
    "    if (x == 0):\n",
    "        y = 'None'\n",
    "    if (x > 0) & (x < 2) :\n",
    "        y = 'Low'\n",
    "    if (x > 1) & (x < 9) :\n",
    "        y = 'Moderate'\n",
    "    if (x > 8) :\n",
    "        y = 'High'\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd372c1",
   "metadata": {
    "id": "fdd372c1"
   },
   "outputs": [],
   "source": [
    "df['Engagement_Total_Class'] = df['Engagement_Total'].apply(engagement_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5eed0",
   "metadata": {
    "id": "63b5eed0"
   },
   "outputs": [],
   "source": [
    "# Plot frequency of users, sources, etc.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_frequency_charts(df, feature, title):\n",
    "    freq_df = pd.DataFrame()\n",
    "    freq_df[feature] = df[feature]\n",
    "    \n",
    "    f, ax = plt.subplots(1,1, figsize=(16,4))\n",
    "    total = float(len(df))\n",
    "    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n",
    "    g.set_title(\"Number and percentage of {}\".format(title))\n",
    "\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(100*height/total),\n",
    "                ha=\"center\") \n",
    "\n",
    "    plt.title('Frequency of {} '.format(feature))\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.xlabel(title, fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ae1b1",
   "metadata": {
    "id": "339ae1b1"
   },
   "outputs": [],
   "source": [
    "#final distribution\n",
    "plot_frequency_charts(df, 'Engagement_Total_Class', 'Engagement_Total_Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393eb70",
   "metadata": {
    "id": "9393eb70"
   },
   "source": [
    "# New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6b418",
   "metadata": {
    "id": "c8f6b418"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "now = datetime.now(tz=pytz.utc)\n",
    "df[\"account_duration\"] = ((now - pd.to_datetime(df[\"author.created_at\"])).dt.total_seconds())/3600\n",
    "df[\"account_duration\"] = df[\"account_duration\"]/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815fda7d",
   "metadata": {
    "id": "815fda7d"
   },
   "outputs": [],
   "source": [
    "df['text_length'] = df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26jvQWq7zoU6",
   "metadata": {
    "id": "26jvQWq7zoU6"
   },
   "outputs": [],
   "source": [
    "df['source'] = df['source'].fillna('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b03c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "e53b03c0",
    "outputId": "cf2e337f-5b27-4c40-c8c0-e7defba16f9d"
   },
   "outputs": [],
   "source": [
    "df[['public_metrics.like_count', 'public_metrics.reply_count', 'public_metrics.retweet_count', 'Engagement_Total']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18deca07",
   "metadata": {
    "id": "18deca07"
   },
   "source": [
    "# Cross-Fold Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e8eb5",
   "metadata": {
    "id": "ff7e8eb5"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a36158",
   "metadata": {
    "id": "e5a36158"
   },
   "outputs": [],
   "source": [
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Target encoder.\n",
    "    \n",
    "    Replaces categorical column(s) with the mean target value for\n",
    "    each category.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cols=None):\n",
    "        \"\"\"Target encoder\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        cols : list of str\n",
    "            Columns to target encode.  Default is to target \n",
    "            encode all categorical columns in the DataFrame.\n",
    "        \"\"\"\n",
    "        if isinstance(cols, str):\n",
    "            self.cols = [cols]\n",
    "        else:\n",
    "            self.cols = cols\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit target encoder to X and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encode all categorical cols by default\n",
    "        if self.cols is None:\n",
    "            self.cols = [col for col in X \n",
    "                         if str(X[col].dtype)=='object']\n",
    "\n",
    "        # Check columns are in X\n",
    "        for col in self.cols:\n",
    "            if col not in X:\n",
    "                raise ValueError('Column \\''+col+'\\' not in X')\n",
    "\n",
    "        # Encode each element of each column\n",
    "        self.maps = dict() #dict to store map for each column\n",
    "        for col in self.cols:\n",
    "            tmap = dict()\n",
    "            uniques = X[col].unique()\n",
    "            for unique in uniques:\n",
    "                tmap[unique] = y[X[col]==unique].mean()\n",
    "            self.maps[col] = tmap\n",
    "            \n",
    "        return self\n",
    "\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Perform the target encoding transformation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        Xo = X.copy()\n",
    "        for col, tmap in self.maps.items():\n",
    "            vals = np.full(X.shape[0], np.nan)\n",
    "            for val, mean_target in tmap.items():\n",
    "                vals[X[col]==val] = mean_target\n",
    "            Xo[col] = vals\n",
    "        return Xo\n",
    "            \n",
    "            \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform the data via target encoding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values (required!).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5818d",
   "metadata": {
    "id": "a1a5818d"
   },
   "outputs": [],
   "source": [
    "class TargetEncoderCV(TargetEncoder):\n",
    "    \"\"\"Cross-fold target encoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=11, shuffle=True, cols=None):\n",
    "        \"\"\"Cross-fold target encoding for categorical features.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_splits : int\n",
    "            Number of cross-fold splits. Default = 3.\n",
    "        shuffle : bool\n",
    "            Whether to shuffle the data when splitting into folds.\n",
    "        cols : list of str\n",
    "            Columns to target encode.\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.cols = cols\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit cross-fold target encoder to X and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        self._target_encoder = TargetEncoder(cols=self.cols)\n",
    "        self._target_encoder.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Perform the target encoding transformation.\n",
    "\n",
    "        Uses cross-fold target encoding for the training fold,\n",
    "        and uses normal target encoding for the test fold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "\n",
    "        # Use target encoding from fit() if this is test data\n",
    "        if y is None:\n",
    "            return self._target_encoder.transform(X)\n",
    "\n",
    "        # Compute means for each fold\n",
    "        self._train_ix = []\n",
    "        self._test_ix = []\n",
    "        self._fit_tes = []\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle)\n",
    "        for train_ix, test_ix in kf.split(X):\n",
    "            self._train_ix.append(train_ix)\n",
    "            self._test_ix.append(test_ix)\n",
    "            te = TargetEncoder(cols=self.cols)\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                self._fit_tes.append(te.fit(X.iloc[train_ix,:],\n",
    "                                            y.iloc[train_ix]))\n",
    "            elif isinstance(X, np.ndarray):\n",
    "                self._fit_tes.append(te.fit(X[train_ix,:],\n",
    "                                            y[train_ix]))\n",
    "            else:\n",
    "                raise TypeError('X must be DataFrame or ndarray')\n",
    "\n",
    "        # Apply means across folds\n",
    "        Xo = X.copy()\n",
    "        for ix in range(len(self._test_ix)):\n",
    "            test_ix = self._test_ix[ix]\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                Xo.iloc[test_ix,:] = \\\n",
    "                    self._fit_tes[ix].transform(X.iloc[test_ix,:])\n",
    "            elif isinstance(X, np.ndarray):\n",
    "                Xo[test_ix,:] = \\\n",
    "                    self._fit_tes[ix].transform(X[test_ix,:])\n",
    "            else:\n",
    "                raise TypeError('X must be DataFrame or ndarray')\n",
    "        return Xo\n",
    "\n",
    "            \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform the data via target encoding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values (required!).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P_0H4sQUrsNs",
   "metadata": {
    "id": "P_0H4sQUrsNs"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d2b69",
   "metadata": {
    "id": "a52d2b69"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22baf51e",
   "metadata": {
    "id": "22baf51e"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88d70b",
   "metadata": {
    "id": "4e88d70b"
   },
   "outputs": [],
   "source": [
    "te = TargetEncoderCV()\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb88db28",
   "metadata": {
    "id": "cb88db28"
   },
   "outputs": [],
   "source": [
    "df_new = df[[\n",
    "        #'possibly_sensitive',\n",
    "        #'time_diff_hours', 'check_poll',\n",
    "         'flesch_reading',\n",
    "         'mention_count','url_count', \n",
    "        # 'check_media', \n",
    "        'check_hashtags',\n",
    "         'avg_length_words',\n",
    "        'emoji_count', 'text_length', \n",
    "        'author.public_metrics.listed_count',\n",
    "        'author.public_metrics.followers_count', 'author.public_metrics.tweet_count', \n",
    "        'profanity_probability', \n",
    "        'account_duration'\n",
    "       ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61d41f",
   "metadata": {
    "id": "ba61d41f"
   },
   "outputs": [],
   "source": [
    "df_new2 = df[[ 'day_name', \n",
    "              'source', \n",
    "        'text_preprocessed_sentiment', 'text_preprocessed_emotion_heading',\n",
    "        'possibly_sensitive',\n",
    "        'check_media'\n",
    "        #, 'check_hashtags'\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984727e",
   "metadata": {
    "id": "6984727e"
   },
   "outputs": [],
   "source": [
    "df_new2 = te.fit_transform(df_new2, df['Engagement_Total']).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967da6e4",
   "metadata": {
    "id": "967da6e4"
   },
   "outputs": [],
   "source": [
    "df_new = pd.merge(df_new, df_new2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9e670",
   "metadata": {
    "id": "caa9e670"
   },
   "outputs": [],
   "source": [
    "#df_new[\"possibly_sensitive\"] = df_new[\"possibly_sensitive\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6av-Gze1h3xR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "6av-Gze1h3xR",
    "outputId": "db004236-0440-4a29-ccdb-97b22fab9ae4"
   },
   "outputs": [],
   "source": [
    "# Check the skew of all numerical features\n",
    "numeric_feats = df_new.dtypes[df_new.dtypes != \"object\"].index\n",
    "skewed_feats = df_new[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vohqjtH0lyh_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vohqjtH0lyh_",
    "outputId": "5af4608a-2fde-4a91-82cc-4a7597553e9d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "pt.fit(df_new)\n",
    "lambdas = pd.DataFrame({'cols':df_new.columns,\"box-cox-lambdas\":pt.lambdas_})\n",
    "print(pt.lambdas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9grll5nogFfX",
   "metadata": {
    "id": "9grll5nogFfX"
   },
   "outputs": [],
   "source": [
    " for col in df_new.columns:\n",
    "        t = pt.fit_transform(np.array(df_new[col]).reshape(-1,1))\n",
    "        df_new[col] = t.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WDVvX7RYl4-O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDVvX7RYl4-O",
    "outputId": "ca985053-11f9-460d-c990-4c111ab7517f"
   },
   "outputs": [],
   "source": [
    "# from scipy.special import boxcox1p\n",
    "# skewness = skewness[abs(skewness) > 0.75]\n",
    "# print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "# from scipy.special import boxcox1p\n",
    "# skewed_features = skewness.index\n",
    "# for feat in skewed_features:    \n",
    "#     df_new[feat] = boxcox1p(df_new[feat], lambdas.loc[lambdas['cols'] == feat, 'box-cox-lambdas'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WE5__Xrml6xm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "WE5__Xrml6xm",
    "outputId": "7e0a9b88-7094-4940-f220-69f48a6a5456"
   },
   "outputs": [],
   "source": [
    "# Check the skew of all numerical features\n",
    "numeric_feats = df_new.dtypes[df_new.dtypes != \"object\"].index\n",
    "skewed_feats = df_new[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258512f2",
   "metadata": {
    "id": "258512f2"
   },
   "outputs": [],
   "source": [
    "X = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05168fa1",
   "metadata": {
    "id": "05168fa1"
   },
   "outputs": [],
   "source": [
    "Y = df['Engagement_Total_Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=5346854)\n",
    "#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1eeba1",
   "metadata": {
    "id": "7d1eeba1"
   },
   "outputs": [],
   "source": [
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a82da",
   "metadata": {
    "id": "c41a82da"
   },
   "source": [
    "# Comparing All the Models without Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89dbbb",
   "metadata": {
    "id": "1a89dbbb"
   },
   "outputs": [],
   "source": [
    "X_test['flesch_reading'] = X_test['flesch_reading'].fillna(0)\n",
    "X_train['flesch_reading'] = X_train['flesch_reading'].fillna(0)\n",
    "X['flesch_reading'] = X['flesch_reading'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TQjtSlrFuVAY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQjtSlrFuVAY",
    "outputId": "8f7853c0-2481-409e-fb0c-bf246a315e84"
   },
   "outputs": [],
   "source": [
    "!pip install CatBoost\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vFzGs4YRq7j5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFzGs4YRq7j5",
    "outputId": "665e358f-4f54-4014-e823-dd03a7c6fdb5"
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45e19f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac45e19f",
    "outputId": "5117ce62-8d31-4404-cfd9-ae113961f0b3"
   },
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "\n",
    "classifiers = [\n",
    "    CatBoostClassifier(),\n",
    "    lgb.LGBMClassifier(),\n",
    "    KNeighborsClassifier(3),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    LogisticRegression()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1zz9YG1h2N-K",
   "metadata": {
    "id": "1zz9YG1h2N-K"
   },
   "outputs": [],
   "source": [
    "\n",
    "import scipy.stats\n",
    "log_cols=[\"Classifier\", 'F1 Score', 'ci_lower', 'ci_upper' ]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "confidence = 0.95  # Change to your desired confidence level\n",
    "z_value = scipy.stats.norm.ppf((1 + confidence) / 2.0)\n",
    "\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    print(name)\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    f1 = f1_score(y_test, train_predictions, average='micro')\n",
    "    ci_length = z_value * np.sqrt((f1 * (1 - f1)) / y_test.shape[0])\n",
    "    ci_lower = f1 - ci_length\n",
    "    ci_upper = f1 + ci_length\n",
    "\n",
    "    print(\"F1 Score: {}\".format(f1))\n",
    "    \n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, f1, ci_lower, ci_upper ]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z3lBpW5ultS9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "Z3lBpW5ultS9",
    "outputId": "37712ef2-e960-48aa-d6a4-1c4d95be73b1"
   },
   "outputs": [],
   "source": [
    "log.reset_index(drop=True)\n",
    "from google.colab import files\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "g = (log['F1 Score'].plot.barh(xerr=log['F1 Score'] - log['ci_lower'],\n",
    "                     ax=ax, capsize=4)\n",
    ")\n",
    "ax.set_yticklabels(log['Classifier'])\n",
    "ax.set_xlim(0.4, 0.605)\n",
    "\n",
    "for i, v in enumerate(log['F1 Score']):\n",
    "    plt.text(v+0.010, i, str(round(v, 3)), color='steelblue', va=\"center\")\n",
    "\n",
    "\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('Classifier F1 Score')\n",
    "plt.savefig(\"all_models.pdf\", bbox_inches='tight')\n",
    "files.download(\"all_models.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1OjE9hb2d_4R",
   "metadata": {
    "id": "1OjE9hb2d_4R"
   },
   "source": [
    "# Bootstrapping Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rQw5iMGFNz3U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQw5iMGFNz3U",
    "outputId": "02a2bb3e-bf67-40b9-bbd3-c22733188c95"
   },
   "outputs": [],
   "source": [
    "#clf = XGBClassifier()\n",
    "#clf = lgb.LGBMClassifier()\n",
    "#clf = CatBoostClassifier()\n",
    "clf = LogisticRegression()\n",
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(y_train.shape[0])\n",
    "X_train2 = X_train.reset_index(drop=True)\n",
    "y_train2 = y_train.reset_index(drop=True)\n",
    "bootstrap_train_accuracies = []\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    train_idx = rng.choice(idx, size=idx.shape[0], replace=True)\n",
    "    test_idx = np.setdiff1d(idx, train_idx, assume_unique=False)\n",
    "    \n",
    "    \n",
    "    boot_train_X, boot_train_y = X_train2.loc[train_idx], y_train2.loc[train_idx]\n",
    "    boot_test_X, boot_test_y = X_train2.loc[test_idx], y_train2.loc[test_idx]\n",
    "\n",
    "    clf.fit(boot_train_X, boot_train_y)\n",
    "    train_predictions = clf.predict(boot_test_X)\n",
    "    acc = f1_score(boot_test_y, train_predictions, average='micro')\n",
    "    bootstrap_train_accuracies.append(acc)\n",
    "\n",
    "bootstrap_train_mean = np.mean(bootstrap_train_accuracies)\n",
    "bootstrap_train_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yZagvs0eZs1Q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZagvs0eZs1Q",
    "outputId": "752ab33e-3c96-4a19-8582-2bdd6d1f9b7a"
   },
   "outputs": [],
   "source": [
    "bootstrap_rounds = 200\n",
    "confidence = 0.95  # Change to your desired confidence level\n",
    "t_value = scipy.stats.t.ppf((1 + confidence) / 2.0, df=bootstrap_rounds - 1)\n",
    "se = 0.0\n",
    "for acc in bootstrap_train_accuracies:\n",
    "    se += (acc - bootstrap_train_mean) ** 2\n",
    "se = np.sqrt((1.0 / (bootstrap_rounds - 1)) * se)\n",
    "\n",
    "ci_length = t_value * se\n",
    "\n",
    "ci_lower = bootstrap_train_mean - ci_length\n",
    "ci_upper = bootstrap_train_mean + ci_length\n",
    "\n",
    "print(ci_lower, ci_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLNtXpbdZ2ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "aLNtXpbdZ2ca",
    "outputId": "fb76ecfe-7d75-422e-dd59-6137c8d845db"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.vlines( bootstrap_train_mean, [0], 80, lw=2.5, linestyle='-', label='bootstrap train mean')\n",
    "\n",
    "ax.vlines(ci_upper, [0], 15, lw=2.5, linestyle='dotted', \n",
    "          label='CI95 bootstrap', color='C2')\n",
    "ax.vlines(ci_lower, [0], 15, lw=2.5, linestyle='dotted', color='C2')\n",
    "\n",
    "ax.hist(bootstrap_train_accuracies, bins=7,\n",
    "        color='#0080ff', edgecolor=\"none\", \n",
    "        alpha=0.3)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.xlim([0.55, 0.5675])\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.title('Logistic Regression F1 Score')\n",
    "plt.savefig(\"lr3_ci.pdf\", bbox_inches='tight')\n",
    "files.download(\"lr3_ci.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HdWPPW10rEUb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdWPPW10rEUb",
    "outputId": "44c98b9b-6ca1-484b-9b3d-84db50dc8791"
   },
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(y_train.shape[0])\n",
    "X_train2 = X_train.reset_index(drop=True)\n",
    "y_train2 = y_train.reset_index(drop=True)\n",
    "bootstrap_train_accuracies = []\n",
    "\n",
    "for i in range(30):\n",
    "    \n",
    "    train_idx = rng.choice(idx, size=idx.shape[0], replace=True)\n",
    "    test_idx = np.setdiff1d(idx, train_idx, assume_unique=False)\n",
    "    \n",
    "    \n",
    "    boot_train_X, boot_train_y = X_train2.loc[train_idx], y_train2.loc[train_idx]\n",
    "    boot_test_X, boot_test_y = X_train2.loc[test_idx], y_train2.loc[test_idx]\n",
    "\n",
    "    clf.fit(boot_train_X, boot_train_y)\n",
    "    train_predictions = clf.predict(boot_test_X)\n",
    "    acc = f1_score(boot_test_y, train_predictions, average='micro')\n",
    "    bootstrap_train_accuracies.append(acc)\n",
    "\n",
    "bootstrap_train_mean = np.mean(bootstrap_train_accuracies)\n",
    "bootstrap_train_mean\n",
    "bootstrap_rounds = 30\n",
    "confidence = 0.95  # Change to your desired confidence level\n",
    "t_value = scipy.stats.t.ppf((1 + confidence) / 2.0, df=bootstrap_rounds - 1)\n",
    "se = 0.0\n",
    "for acc in bootstrap_train_accuracies:\n",
    "    se += (acc - bootstrap_train_mean) ** 2\n",
    "se = np.sqrt((1.0 / (bootstrap_rounds - 1)) * se)\n",
    "\n",
    "ci_length = t_value * se\n",
    "\n",
    "ci_lower = bootstrap_train_mean - ci_length\n",
    "ci_upper = bootstrap_train_mean + ci_length\n",
    "\n",
    "print(ci_lower, ci_upper)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ic2sKXhmzV5m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "ic2sKXhmzV5m",
    "outputId": "3694ff11-0af9-437d-cdf2-dc82035272e9"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.vlines( bootstrap_train_mean, [0], 30, lw=2.5, linestyle='-', label='bootstrap train mean')\n",
    "\n",
    "ax.vlines(ci_upper, [0], 15, lw=2.5, linestyle='dotted', \n",
    "          label='CI95 bootstrap', color='C2')\n",
    "ax.vlines(ci_lower, [0], 15, lw=2.5, linestyle='dotted', color='C2')\n",
    "\n",
    "ax.hist(bootstrap_train_accuracies, bins=7,\n",
    "        color='#0080ff', edgecolor=\"none\", \n",
    "        alpha=0.3)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.xlim([0.55, 0.58])\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.title('XGBoost F1 Score')\n",
    "plt.savefig(\"xg2_ci.pdf\", bbox_inches='tight')\n",
    "files.download(\"xg2_ci.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9jDA1znartR_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jDA1znartR_",
    "outputId": "71737a2d-d28c-445a-cba7-a597c499ed65"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(y_train.shape[0])\n",
    "X_train2 = X_train.reset_index(drop=True)\n",
    "y_train2 = y_train.reset_index(drop=True)\n",
    "bootstrap_train_accuracies = []\n",
    "\n",
    "for i in range(30):\n",
    "    \n",
    "    train_idx = rng.choice(idx, size=idx.shape[0], replace=True)\n",
    "    test_idx = np.setdiff1d(idx, train_idx, assume_unique=False)\n",
    "    \n",
    "    \n",
    "    boot_train_X, boot_train_y = X_train2.loc[train_idx], y_train2.loc[train_idx]\n",
    "    boot_test_X, boot_test_y = X_train2.loc[test_idx], y_train2.loc[test_idx]\n",
    "\n",
    "    clf.fit(boot_train_X, boot_train_y)\n",
    "    train_predictions = clf.predict(boot_test_X)\n",
    "    acc = f1_score(boot_test_y, train_predictions, average='micro')\n",
    "    bootstrap_train_accuracies.append(acc)\n",
    "\n",
    "bootstrap_train_mean = np.mean(bootstrap_train_accuracies)\n",
    "bootstrap_train_mean\n",
    "bootstrap_rounds = 30\n",
    "confidence = 0.95  # Change to your desired confidence level\n",
    "t_value = scipy.stats.t.ppf((1 + confidence) / 2.0, df=bootstrap_rounds - 1)\n",
    "se = 0.0\n",
    "for acc in bootstrap_train_accuracies:\n",
    "    se += (acc - bootstrap_train_mean) ** 2\n",
    "se = np.sqrt((1.0 / (bootstrap_rounds - 1)) * se)\n",
    "\n",
    "ci_length = t_value * se\n",
    "\n",
    "ci_lower = bootstrap_train_mean - ci_length\n",
    "ci_upper = bootstrap_train_mean + ci_length\n",
    "\n",
    "print(ci_lower, ci_upper)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knsyenJM2Vad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "knsyenJM2Vad",
    "outputId": "e98598f5-85a9-4bc5-c3c5-ff32fdf669af"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.vlines( bootstrap_train_mean, [0], 30, lw=2.5, linestyle='-', label='bootstrap train mean')\n",
    "\n",
    "ax.vlines(ci_upper, [0], 15, lw=2.5, linestyle='dotted', \n",
    "          label='CI95 bootstrap', color='C2')\n",
    "ax.vlines(ci_lower, [0], 15, lw=2.5, linestyle='dotted', color='C2')\n",
    "\n",
    "ax.hist(bootstrap_train_accuracies, bins=7,\n",
    "        color='#0080ff', edgecolor=\"none\", \n",
    "        alpha=0.3)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.xlim([0.56, 0.58])\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.title('LightGBM F1 Score')\n",
    "plt.savefig(\"gbm2_ci.pdf\", bbox_inches='tight')\n",
    "files.download(\"gbm2_ci.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2oen7tj-faX8",
   "metadata": {
    "id": "2oen7tj-faX8"
   },
   "outputs": [],
   "source": [
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\", 'F1 Score']\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    train_predictions = clf.predict(X_test)\n",
    "    f1 = f1_score(y_test, train_predictions, average='micro')\n",
    "    print(\"F1 Score: {}\".format(f1))\n",
    "    \n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll, f1]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2c91c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "25d2c91c",
    "outputId": "26af79bf-6c77-468e-bf6d-53d92876dc32"
   },
   "outputs": [],
   "source": [
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n",
    "\n",
    "plt.xlabel('Accuracy %')\n",
    "plt.title('Classifier Accuracy')\n",
    "plt.show()\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n",
    "\n",
    "plt.xlabel('Log Loss')\n",
    "plt.title('Classifier Log Loss')\n",
    "plt.show()\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='F1 Score', y='Classifier', data=log, color=\"r\")\n",
    "\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('Classifier F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5vI_G_ZfYv4T",
   "metadata": {
    "id": "5vI_G_ZfYv4T"
   },
   "source": [
    "# Grouping the feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kPw-lHIOYBkC",
   "metadata": {
    "id": "kPw-lHIOYBkC"
   },
   "outputs": [],
   "source": [
    "log_cols=[\"Feature Group\", \"F1 Score\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pq_JZnmTY3MA",
   "metadata": {
    "id": "pq_JZnmTY3MA"
   },
   "outputs": [],
   "source": [
    "X_lexical = df_new[['mention_count', 'url_count', 'avg_length_words',\n",
    "                        'emoji_count', 'text_length', 'possibly_sensitive', 'check_hashtags',  'check_media'\t\n",
    "                        ]]\n",
    "Y_lexical = df['Engagement_Total_Class']\n",
    "X_train_lexical, X_test_lexical, y_train_lexical, y_test_lexical = train_test_split(X_lexical, Y_lexical, test_size=0.3, random_state=5346854)\n",
    "\n",
    "\n",
    "X_metadata = df_new[['author.public_metrics.listed_count', \n",
    "                   'author.public_metrics.followers_count', \n",
    "                   'author.public_metrics.tweet_count','account_duration', \n",
    "                   'day_name', 'source']]\n",
    "Y_metadata = df['Engagement_Total_Class']\n",
    "X_train_metadata, X_test_metadata, y_train_metadata, y_test_metadata = train_test_split(X_metadata, Y_metadata, test_size=0.3, random_state=5346854)\n",
    "\n",
    "X_NLP =df_new[['text_preprocessed_sentiment', \n",
    "                   'text_preprocessed_emotion_heading', 'flesch_reading', 'profanity_probability']]\n",
    "Y_NLP = df['Engagement_Total_Class']\n",
    "X_train_NLP, X_test_NLP, y_train_NLP, y_test_NLP = train_test_split(X_NLP, Y_NLP, test_size=0.3, random_state=5346854)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wj-xjvBmR0Io",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wj-xjvBmR0Io",
    "outputId": "1b7bc29e-be49-4004-c5d4-8e6b2ecd6175"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier( n_estimators= 198,\n",
    "                          learning_rate = 0.05, \n",
    "                          num_leaves = 41,\n",
    "                         lambda_l2 =0.0705, \n",
    "                          feature_fraction = 0.6154918829220486, \n",
    "                          bagging_fraction= 0.6096119515891867, \n",
    "                          bagging_freq =1,\n",
    "                          min_data_in_leaf= 4, \n",
    "                         min_split_gain =0.0169,\n",
    "                         reg_alpha = 0.001,\n",
    "    )\n",
    "clf.fit(X_train_lexical, y_train_lexical)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test_lexical)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test_lexical, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))\n",
    "\n",
    "log = log.append(pd.DataFrame([['Tweet', f1]], columns=log_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I7qlqJIISFDs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7qlqJIISFDs",
    "outputId": "4d8e1630-d95f-4595-9a44-72fe91479cf5"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier( n_estimators= 198,\n",
    "                          learning_rate = 0.05, \n",
    "                          num_leaves = 41,\n",
    "                         lambda_l2 =0.0705, \n",
    "                          feature_fraction = 0.6154918829220486, \n",
    "                          bagging_fraction= 0.6096119515891867, \n",
    "                          bagging_freq =1,\n",
    "                          min_data_in_leaf= 4, \n",
    "                         min_split_gain =0.0169,\n",
    "                         reg_alpha = 0.001,\n",
    "    )\n",
    "clf.fit(X_train_metadata, y_train_metadata)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test_metadata)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test_metadata, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))\n",
    "log = log.append(pd.DataFrame([['Metadata', f1]], columns=log_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q32jJm8qSLdl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q32jJm8qSLdl",
    "outputId": "9f4edfd2-a854-4f77-8598-1ebdb9ed5c6f"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier( n_estimators= 198,\n",
    "                          learning_rate = 0.05, \n",
    "                          num_leaves = 41,\n",
    "                         lambda_l2 =0.0705, \n",
    "                          feature_fraction = 0.6154918829220486, \n",
    "                          bagging_fraction= 0.6096119515891867, \n",
    "                          bagging_freq =1,\n",
    "                          min_data_in_leaf= 4, \n",
    "                         min_split_gain =0.0169,\n",
    "                         reg_alpha = 0.001,\n",
    "    )\n",
    "clf.fit(X_train_NLP, y_train_NLP)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test_NLP)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test_NLP, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))\n",
    "\n",
    "log = log.append(pd.DataFrame([['Text', f1]], columns=log_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WEQVaGAwlBZm",
   "metadata": {
    "id": "WEQVaGAwlBZm"
   },
   "outputs": [],
   "source": [
    "X_NLP_met =df_new[['author.public_metrics.listed_count', \n",
    "                   'author.public_metrics.followers_count', \n",
    "                   'author.public_metrics.tweet_count','account_duration', \n",
    "                   'day_name', 'source',\n",
    "               'text_preprocessed_sentiment', \n",
    "                   'text_preprocessed_emotion_heading', 'flesch_reading', 'profanity_probability'\n",
    "               ]]\n",
    "Y_NLP_met = df['Engagement_Total_Class']\n",
    "X_train_NLP_met, X_test_NLP_met, y_train_NLP_met, y_test_NLP_met = train_test_split(X_NLP_met, Y_NLP_met, test_size=0.3, random_state=5346854)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "THN3jRB_S1Nd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THN3jRB_S1Nd",
    "outputId": "6014a00e-8346-4792-9c44-c0c3e94afed5"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier( n_estimators= 198,\n",
    "                          learning_rate = 0.05, \n",
    "                          num_leaves = 41,\n",
    "                         lambda_l2 =0.0705, \n",
    "                          feature_fraction = 0.6154918829220486, \n",
    "                          bagging_fraction= 0.6096119515891867, \n",
    "                          bagging_freq =1,\n",
    "                          min_data_in_leaf= 4, \n",
    "                         min_split_gain =0.0169,\n",
    "                         reg_alpha = 0.001,\n",
    "    )\n",
    "clf.fit(X_train_NLP_met, y_train_NLP_met)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test_NLP_met)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test_NLP_met, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))\n",
    "log = log.append(pd.DataFrame([['Text + Metadata', f1]], columns=log_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UR5rp1JGS9eG",
   "metadata": {
    "id": "UR5rp1JGS9eG"
   },
   "outputs": [],
   "source": [
    "X_lex_met =df_new[['author.public_metrics.listed_count', \n",
    "                   'author.public_metrics.followers_count', \n",
    "                   'author.public_metrics.tweet_count','account_duration', \n",
    "                   'day_name', 'source',\n",
    "              'mention_count', 'url_count', 'avg_length_words',\n",
    "                        'emoji_count', 'text_length', 'possibly_sensitive', 'check_hashtags',  'check_media'\t\n",
    "               ]]\n",
    "Y_lex_met = df['Engagement_Total_Class']\n",
    "X_train_lex_met, X_test_lex_met, y_train_lex_met, y_test_lex_met = train_test_split(X_lex_met, Y_lex_met, test_size=0.3, random_state=5346854)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8r90Cwn9TIOY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8r90Cwn9TIOY",
    "outputId": "4acbcafe-f64e-4903-dac0-875245840b79"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier( n_estimators= 198,\n",
    "                          learning_rate = 0.05, \n",
    "                          num_leaves = 41,\n",
    "                         lambda_l2 =0.0705, \n",
    "                          feature_fraction = 0.6154918829220486, \n",
    "                          bagging_fraction= 0.6096119515891867, \n",
    "                          bagging_freq =1,\n",
    "                          min_data_in_leaf= 4, \n",
    "                         min_split_gain =0.0169,\n",
    "                         reg_alpha = 0.001,\n",
    "    )\n",
    "clf.fit(X_train_lex_met, y_train_lex_met)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test_lex_met)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test_lex_met, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))\n",
    "log = log.append(pd.DataFrame([['Tweet + Metadata', f1]], columns=log_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jOV6-srITnvU",
   "metadata": {
    "id": "jOV6-srITnvU"
   },
   "outputs": [],
   "source": [
    "X_all =df_new[['author.public_metrics.listed_count', \n",
    "                   'author.public_metrics.followers_count', \n",
    "                   'author.public_metrics.tweet_count','account_duration', \n",
    "                   'day_name', 'source',\n",
    "              'mention_count', 'url_count', 'avg_length_words',\n",
    "                        'emoji_count', 'text_length', 'possibly_sensitive', 'check_hashtags',  'check_media'\t,\n",
    "               'text_preprocessed_sentiment', \n",
    "                   'text_preprocessed_emotion_heading', 'flesch_reading', 'profanity_probability'\n",
    "               ]]\n",
    "Y_all = df['Engagement_Total_Class']\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_all, Y_all, test_size=0.3, random_state=5346854)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JERes5ZmT7l8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JERes5ZmT7l8",
    "outputId": "041102ac-5746-4aff-d99f-234807050cda"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier( n_estimators= 198,\n",
    "                          learning_rate = 0.05, \n",
    "                          num_leaves = 41,\n",
    "                         lambda_l2 =0.0705, \n",
    "                          feature_fraction = 0.6154918829220486, \n",
    "                          bagging_fraction= 0.6096119515891867, \n",
    "                          bagging_freq =1,\n",
    "                          min_data_in_leaf= 4, \n",
    "                         min_split_gain =0.0169,\n",
    "                         reg_alpha = 0.001,\n",
    "    )\n",
    "clf.fit(X_train_all, y_train_all)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test_all)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test_all, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))\n",
    "\n",
    "log = log.append(pd.DataFrame([['Tweet + Text + Metadata', f1]], columns=log_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5zgWPLV7o8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "dc5zgWPLV7o8",
    "outputId": "45caf1e2-6a28-4e20-f2ac-a247bb7058a7"
   },
   "outputs": [],
   "source": [
    "log.reset_index(drop=True)\n",
    "log.sort_values('F1 Score',inplace=True)\n",
    "from google.colab import files\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "g = (log['F1 Score'].plot.barh(\n",
    "                     ax=ax)\n",
    ")\n",
    "ax.set_yticklabels(log['Feature Group'])\n",
    "ax.set_xlim(0, 0.7)\n",
    "\n",
    "for i, v in enumerate(log['F1 Score']):\n",
    "    plt.text(v+0.010, i, str(round(v, 4)), color='steelblue', va=\"center\")\n",
    "\n",
    "\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Feature Sets')\n",
    "plt.title('LightGBM F1 Scores with Different Feature Sets')\n",
    "plt.savefig(\"feature_sets.pdf\", bbox_inches='tight')\n",
    "files.download(\"feature_sets.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720ea8c",
   "metadata": {
    "id": "c720ea8c"
   },
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a9ba3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e62a9ba3",
    "outputId": "281ddfab-f3bc-412a-c12b-98c4c48e7469",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install CatBoost\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd909f7e",
   "metadata": {
    "id": "cd909f7e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59448834",
   "metadata": {
    "id": "59448834"
   },
   "outputs": [],
   "source": [
    "CBC = CatBoostClassifier()\n",
    "parameters = {'depth'         : [7,8,9, 10],\n",
    "                 'learning_rate' : [0.01,0.05, 0.07, 0.09],\n",
    "                  'iterations'    : [100]\n",
    "             # 'loss_function' :  ['f1_score']\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VE61ejf1DajA",
   "metadata": {
    "id": "VE61ejf1DajA"
   },
   "outputs": [],
   "source": [
    "Grid_CBC = GridSearchCV(estimator=CBC, param_grid = parameters, cv = 5)\n",
    "Grid_CBC.fit(X_train, y_train)\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",Grid_CBC.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",Grid_CBC.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",Grid_CBC.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac0664",
   "metadata": {
    "id": "95ac0664"
   },
   "outputs": [],
   "source": [
    "Grid_CBC = GridSearchCV(estimator=CBC, param_grid = parameters, cv = 5)\n",
    "Grid_CBC.fit(X_train, y_train)\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",Grid_CBC.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",Grid_CBC.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",Grid_CBC.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S9R0A-e-h8c7",
   "metadata": {
    "id": "S9R0A-e-h8c7"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    params = {'iterations':trial.suggest_int(\"iterations\", 100, 500),\n",
    "              #'od_wait':trial.suggest_int('od_wait', 500, 3200),\n",
    "             'loss_function':'MultiClass',\n",
    "              'eval_metric':'TotalF1',\n",
    "              'leaf_estimation_method':'Newton',\n",
    "              'bootstrap_type': 'Bernoulli',\n",
    "              'learning_rate' : trial.suggest_uniform('learning_rate',0.01,0.5),\n",
    "              'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n",
    "              #'subsample': trial.suggest_uniform('subsample',0,1),\n",
    "              #'random_strength': trial.suggest_uniform('random_strength',10,30),\n",
    "              'depth': trial.suggest_int('depth',1,10),\n",
    "              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
    "              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,8),\n",
    "               }\n",
    "    model = CatBoostClassifier(**params)  \n",
    "    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n",
    "        \n",
    "    y_preds = model.predict(X_test)\n",
    "\n",
    "\n",
    "    f1 = f1_score(y_test, y_preds, average= 'micro')\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-mAEZ2ZVonMG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mAEZ2ZVonMG",
    "outputId": "d38308f0-41e5-46f2-8fd5-82f6777fae87"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MsffiHOQtmlR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "MsffiHOQtmlR",
    "outputId": "0ad0ef27-62a5-4ba4-bca8-4e3719eba86c"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v9oXP2uYm7hz",
   "metadata": {
    "id": "v9oXP2uYm7hz"
   },
   "outputs": [],
   "source": [
    "cat_model = CatBoostClassifier( \n",
    "    iterations= 389,\n",
    "    #od_wait= 1996,\n",
    "    learning_rate= 0.1,\n",
    "    reg_lambda= 56.499561470376044,\n",
    "    depth= 5,\n",
    "    #min_data_in_leaf= 18,\n",
    "    leaf_estimation_iterations= 5,\n",
    "    eval_metric = \"TotalF1\",\n",
    "    random_state =42\n",
    "                                                          )\n",
    "#1.06025346 2.2145116 1.7237214, 0.49379734\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "#predict on test set\n",
    "test_predictions = cat_model.predict(X_test)\n",
    "\n",
    "print( f1_score(y_test, test_predictions, average= 'micro') )\n",
    "#calculate test score\n",
    "#cat_y_test = [(x) for x in y_test]\n",
    "#score = cat_model.score(X_test, list(cat_y_test))\n",
    "#print(score)\n",
    "\n",
    "#irony ile 0.53579 dan 0.5376ya yükseldi \n",
    "#0.5741145605596851den  0.5784324442501093 yüksel permutation ile\n",
    "#'depth': 8, 'iterations': 700, 'learning_rate': 0.03\n",
    "#depth = 8, iterations = 500, learning_rate = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "io28en-f_yog",
   "metadata": {
    "id": "io28en-f_yog"
   },
   "outputs": [],
   "source": [
    "# Installing the most recent version of skopt directly from Github\n",
    "!pip install git+https://github.com/scikit-optimize/scikit-optimize.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ky6AmUQtIC6p",
   "metadata": {
    "id": "Ky6AmUQtIC6p"
   },
   "outputs": [],
   "source": [
    "cat_model = CatBoostClassifier(  depth = 7,  learning_rate = 0.09,\n",
    "                               eval_metric = \"TotalF1\"\n",
    "                              # class_weights = {'None': 0.49379734, 'Low': 1.06025346, 'Moderate': 1.7237214, 'High' :2.2145116}\n",
    "                               )\n",
    "#1.06025346 2.2145116 1.7237214, 0.49379734\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "#predict on test set\n",
    "test_predictions = cat_model.predict(X_test)\n",
    "\n",
    "\n",
    "#calculate test score\n",
    "cat_y_test = [(x) for x in y_test]\n",
    "score = cat_model.score(X_test, list(cat_y_test))\n",
    "print(score)\n",
    "\n",
    "#irony ile 0.53579 dan 0.5376ya yükseldi \n",
    "#0.5741145605596851den  0.5784324442501093 yüksel permutation ile\n",
    "#'depth': 8, 'iterations': 700, 'learning_rate': 0.03\n",
    "#depth = 8, iterations = 500, learning_rate = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o_tUNZ2SGCWG",
   "metadata": {
    "id": "o_tUNZ2SGCWG"
   },
   "outputs": [],
   "source": [
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    class_names = df.Engagement_Total_Class.unique()\n",
    "    print(classification_report(y_test, test_predictions, target_names=class_names))\n",
    "\n",
    "    print(confusion_matrix(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19717950",
   "metadata": {
    "id": "19717950"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(Y), y= Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s6tn9c6oDcaP",
   "metadata": {
    "id": "s6tn9c6oDcaP"
   },
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fCmlp-r4DfXm",
   "metadata": {
    "id": "fCmlp-r4DfXm"
   },
   "outputs": [],
   "source": [
    "def objective2(trial):\n",
    "    \n",
    "    params = {'iterations':trial.suggest_int(\"iterations\", 100, 500),\n",
    "              'n_estimators': trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "              'num_leaves':trial.suggest_int('num_leaves', 20, 150),\n",
    "             'loss_function':'MultiClass',\n",
    "              'leaf_estimation_method':'Newton',\n",
    "              'bootstrap_type': 'Bernoulli',\n",
    "              'learning_rate' : trial.suggest_uniform('learning_rate',0.01,0.2),\n",
    "              'lambda_l2': trial.suggest_uniform('reg_lambda',0,1),\n",
    "              'feature_fraction': trial.suggest_uniform('feature_fraction',0,1),\n",
    "              'bagging_fraction': trial.suggest_uniform('bagging_fraction',0,1),\n",
    "              #'subsample': trial.suggest_uniform('subsample',0,1),\n",
    "              #'random_strength': trial.suggest_uniform('random_strength',10,30),\n",
    "              'bagging_freq': trial.suggest_int('bagging_freq',1,10),\n",
    "              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
    "              'min_split_gain': trial.suggest_int('min_split_gain',0,2),\n",
    "              'reg_alpha': trial.suggest_int('reg_alpha',0,0.5),\n",
    "               }\n",
    "    model = XGBClassifier(**params)  \n",
    "    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n",
    "        \n",
    "    y_preds = model.predict(X_test)\n",
    "\n",
    "\n",
    "    f1 = f1_score(y_test, y_preds, average= 'micro')\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9vgT6X32DyXf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vgT6X32DyXf",
    "outputId": "d39e48e5-021a-4188-d8ff-82669f44aa57"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective2, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dXgv3bPHaF9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "2dXgv3bPHaF9",
    "outputId": "f0e0727d-1e55-407d-a44e-afeb4f336a2e"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8xHGi-iaEYWu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xHGi-iaEYWu",
    "outputId": "a4e9968e-8fc4-4163-bc07-21bb350888ca"
   },
   "outputs": [],
   "source": [
    "xg_model = XGBClassifier( \n",
    "    iterations= 257,\n",
    "    n_estimators= 196,\n",
    "    num_leaves= 21,\n",
    "    learning_rate= 0.1373372593055826,\n",
    "    reg_lambda= 0.3425390605318015,\n",
    "    feature_fraction= 0.1763991259677553,\n",
    "    bagging_fraction= 0.34290928133157794,\n",
    "    bagging_freq= 10,\n",
    "    min_data_in_leaf= 5,\n",
    "    min_split_gain= 0,\n",
    "    reg_alpha= 0,\n",
    "                                                          )\n",
    "#1.06025346 2.2145116 1.7237214, 0.49379734\n",
    "xg_model.fit(X_train, y_train)\n",
    "\n",
    "#predict on test set\n",
    "test_predictions = xg_model.predict(X_test)\n",
    "\n",
    "print( f1_score(y_test, test_predictions, average= 'micro') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1ce44",
   "metadata": {
    "id": "e2e1ce44"
   },
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9LhDJHHxFGI",
   "metadata": {
    "id": "i9LhDJHHxFGI"
   },
   "outputs": [],
   "source": [
    "# build the lightgbm model\n",
    "!pip install lightgbm\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NhVX1FPC9UV6",
   "metadata": {
    "id": "NhVX1FPC9UV6"
   },
   "outputs": [],
   "source": [
    "!pip install optuna\n",
    "import optuna  # pip install optuna\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vuK1h7VKRicn",
   "metadata": {
    "id": "vuK1h7VKRicn"
   },
   "outputs": [],
   "source": [
    "!pip install mljar-supervised\n",
    "from supervised.automl import AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ilTFbO-yPEmQ",
   "metadata": {
    "id": "ilTFbO-yPEmQ"
   },
   "outputs": [],
   "source": [
    "# train models with AutoML\n",
    "automl = AutoML(\n",
    "    mode=\"Optuna\",\n",
    "    algorithms=[\"LightGBM\"],\n",
    "    optuna_time_budget=6000, # 10 minutes for tuning \n",
    "    eval_metric=\"f1\"\n",
    ")\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "# compute the accuracy on test data\n",
    "predictions = automl.predict_all(X_test)\n",
    "print(predictions.head())\n",
    "#Trial 50 is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UE8li1QCXTV-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE8li1QCXTV-",
    "outputId": "b627532e-7f56-4a13-9857-eaa145d533db"
   },
   "outputs": [],
   "source": [
    "print(\"Test accuracy:\", f1_score(y_test, predictions[\"label\"], average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TzQzxQeFArCZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzQzxQeFArCZ",
    "outputId": "34add8d2-27ba-471c-b832-d7c6b6770171"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier( )\n",
    "clf.fit(X_train, y_train)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))\n",
    "#F1 Score: 0.5844648853847277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ba9d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d81ba9d4",
    "outputId": "46a66101-e3a3-4446-c6f3-3057d38b2293"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier( n_estimators= 198,\n",
    "                          #max_depth = -1,\n",
    "                          #is_unbalance = True,\n",
    "                         #scale_pos_weight = 99,\n",
    "                          learning_rate = 0.05, \n",
    "                          num_leaves = 41,\n",
    "                          #lambda_l1 = 5.66256982759675,  \n",
    "                          #lambda_l2 =0.08290494108198049, \n",
    "                         lambda_l2 =0.0705, \n",
    "                          feature_fraction = 0.6154918829220486, \n",
    "                          bagging_fraction= 0.6096119515891867, \n",
    "                          bagging_freq =1,\n",
    "                          min_data_in_leaf= 4, \n",
    "                          #extra_trees= True,\n",
    "                         min_split_gain =0.0169,\n",
    "                         #colsample_bytree =3,\n",
    "                         #subsample_freq = 100,\n",
    "                         reg_alpha = 0.001,\n",
    "                         #n_jobs=-1 ,\n",
    "                         #min_child_weight=0.01,\n",
    "                        #max_bin = 255,  # Number of bucketed bin for feature values\n",
    "                       #subsample = 0.6,  # Subsample ratio of the training instance.\n",
    "                       #subsample_freq = 0, \n",
    "\n",
    "                         )\n",
    "clf.fit(X_train, y_train)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EnNOwyjDa0uK",
   "metadata": {
    "id": "EnNOwyjDa0uK"
   },
   "outputs": [],
   "source": [
    "clf_lgb = lgb.LGBMClassifier( n_estimators= 198,\n",
    "                          #max_depth = -1,\n",
    "                          #is_unbalance = True,\n",
    "                         #scale_pos_weight = 99,\n",
    "                          learning_rate = 0.05, \n",
    "                          num_leaves = 41,\n",
    "                          #lambda_l1 = 5.66256982759675,  \n",
    "                          #lambda_l2 =0.08290494108198049, \n",
    "                         lambda_l2 =0.0705, \n",
    "                          feature_fraction = 0.6154918829220486, \n",
    "                          bagging_fraction= 0.6096119515891867, \n",
    "                          bagging_freq =1,\n",
    "                          min_data_in_leaf= 4, \n",
    "                          #extra_trees= True,\n",
    "                         min_split_gain =0.0169,\n",
    "                         #colsample_bytree =3,\n",
    "                         #subsample_freq = 100,\n",
    "                         reg_alpha = 0.001,\n",
    "                         #n_jobs=-1 ,\n",
    "                         #min_child_weight=0.01,\n",
    "                        #max_bin = 255,  # Number of bucketed bin for feature values\n",
    "                       #subsample = 0.6,  # Subsample ratio of the training instance.\n",
    "                       #subsample_freq = 0, \n",
    "\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23xcfvgvy0Ty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23xcfvgvy0Ty",
    "outputId": "3101dc82-a5c5-41ed-f617-80f51b25ff8a"
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "                         #n_jobs=-1 ,\n",
    "                         #min_child_weight=0.01,\n",
    "                        #max_bin = 255,  # Number of bucketed bin for feature values\n",
    "                       #subsample = 0.6,  # Subsample ratio of the training instance.\n",
    "                       #subsample_freq = 0, \n",
    "                       iterations= 413,\n",
    "                        n_estimators= 168,\n",
    "                        num_leaves= 148,\n",
    "                        learning_rate= 0.07481419023974925,\n",
    "                        reg_lambda= 0.6929010406626589,\n",
    "                        feature_fraction= 0.680234490925464,\n",
    "                        bagging_fraction= 0.9986971230650716,\n",
    "                        bagging_freq= 10,\n",
    "                        min_data_in_leaf= 1,\n",
    "                        min_split_gain= 2,\n",
    "                        reg_alpha= 0,\n",
    "\n",
    "                         )\n",
    "clf.fit(X_train, y_train)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lzzLD_77xExg",
   "metadata": {
    "id": "lzzLD_77xExg"
   },
   "outputs": [],
   "source": [
    "def objective2(trial):\n",
    "    \n",
    "    params = {'iterations':trial.suggest_int(\"iterations\", 100, 500),\n",
    "              'n_estimators': trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "              'num_leaves':trial.suggest_int('num_leaves', 20, 150),\n",
    "             'loss_function':'MultiClass',\n",
    "              'eval_metric':'TotalF1',\n",
    "              'leaf_estimation_method':'Newton',\n",
    "              'bootstrap_type': 'Bernoulli',\n",
    "              'learning_rate' : trial.suggest_uniform('learning_rate',0.01,0.2),\n",
    "              'lambda_l2': trial.suggest_uniform('reg_lambda',0,1),\n",
    "              'feature_fraction': trial.suggest_uniform('feature_fraction',0,1),\n",
    "              'bagging_fraction': trial.suggest_uniform('bagging_fraction',0,1),\n",
    "              #'subsample': trial.suggest_uniform('subsample',0,1),\n",
    "              #'random_strength': trial.suggest_uniform('random_strength',10,30),\n",
    "              'bagging_freq': trial.suggest_int('bagging_freq',1,10),\n",
    "              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
    "              'min_split_gain': trial.suggest_int('min_split_gain',0,2),\n",
    "              'reg_alpha': trial.suggest_int('reg_alpha',0,0.5),\n",
    "               }\n",
    "    model = lgb.LGBMClassifier(**params)  \n",
    "    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n",
    "        \n",
    "    y_preds = model.predict(X_test)\n",
    "\n",
    "\n",
    "    f1 = f1_score(y_test, y_preds, average= 'micro')\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jsK8Sa2lx9CY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsK8Sa2lx9CY",
    "outputId": "447a7e27-d634-4219-fc9c-1f9065d86719"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective2, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6vo4xmRIzOj_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "6vo4xmRIzOj_",
    "outputId": "78cf50aa-2528-40b8-8023-c3a62be55e14"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Allxa-FKZft1",
   "metadata": {
    "id": "Allxa-FKZft1"
   },
   "source": [
    "# Stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EE9dianFaQ84",
   "metadata": {
    "id": "EE9dianFaQ84"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XfLXCFJ2ZjHh",
   "metadata": {
    "id": "XfLXCFJ2ZjHh"
   },
   "outputs": [],
   "source": [
    "classifiers = [('lgbm', clf_lgb),      \n",
    "               #('xgb', xg_model),\n",
    "                                      \n",
    "               ('cat', cat_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DlOXIF_Y6NUx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlOXIF_Y6NUx",
    "outputId": "bac3f538-da44-4ce8-9e8d-5a283d8fcb0f"
   },
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9tEsJTM7Ji8",
   "metadata": {
    "id": "a9tEsJTM7Ji8"
   },
   "outputs": [],
   "source": [
    "clf_lgb\n",
    "est2 = clf_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7sg2gsgW9fAT",
   "metadata": {
    "id": "7sg2gsgW9fAT"
   },
   "outputs": [],
   "source": [
    "#pip install pdpbox\n",
    "import pandas as pd\n",
    "from pdpbox import pdp, get_dataset, info_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nbH7yQvfn2dd",
   "metadata": {
    "id": "nbH7yQvfn2dd"
   },
   "outputs": [],
   "source": [
    "clf3 = StackingClassifier(estimators=classifiers ,\n",
    "                  final_estimator=lgb.LGBMClassifier()\n",
    "                \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AdQP0wYxoScD",
   "metadata": {
    "id": "AdQP0wYxoScD"
   },
   "outputs": [],
   "source": [
    "clf3.fit(X_train, y_train)\n",
    "# predict the results\n",
    "y_pred=clf3.predict(X_test)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xfL0zCI-Z8Hg",
   "metadata": {
    "id": "xfL0zCI-Z8Hg"
   },
   "outputs": [],
   "source": [
    "clf = StackingClassifier(estimators=classifiers \n",
    "                  use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=lr\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9l0EvbsaA2H",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k9l0EvbsaA2H"
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test)\n",
    "# view accuracy\n",
    "f1 = f1_score (y_test, y_pred, average='micro')\n",
    "print(\"F1 Score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZjktNYaJdWJM",
   "metadata": {
    "id": "ZjktNYaJdWJM"
   },
   "outputs": [],
   "source": [
    "F1 Score: 0.5863994743758213"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8391e2",
   "metadata": {
    "id": "ca8391e2"
   },
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13e415",
   "metadata": {
    "id": "cf13e415"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance,names,model_type):\n",
    "    \n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "    \n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "    \n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    \n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "    #Add chart labels\n",
    "    plt.title(model_type + 'FEATURE IMPORTANCE')\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4IGMzHvFLMGS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "4IGMzHvFLMGS",
    "outputId": "0e141f9d-894a-4ab2-93d3-aebaec5597f9"
   },
   "outputs": [],
   "source": [
    "#plot the catboost result\n",
    "plot_feature_importance(clf.feature_importances_,X_train.columns,'LIGHTGBM ')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Results.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
