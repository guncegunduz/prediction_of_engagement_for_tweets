{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cdeae9a",
   "metadata": {},
   "source": [
    "# Twarc Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#twarc is a command line tool and Python library for collecting and archiving Twitter JSON data via the Twitter API\n",
    "#!pip install twarc-csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from twarc.client2 import Twarc2\n",
    "from twarc_csv import CSVConverter\n",
    "\n",
    "# Bearer token from Twitter Developer should be here\n",
    "t = Twarc2(bearer_token=\"XXX\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(10000,1500,-40):\n",
    "\n",
    "    # Start and end times must be in UTC\n",
    "    start_time = datetime.now(timezone.utc) + timedelta(minutes=-x)\n",
    "    x = x - 5\n",
    "    # end_time must to be at least 30 seconds ago.\n",
    "    end_time = datetime.now(timezone.utc) + timedelta(minutes=-x)\n",
    "\n",
    "    #a, the i, you are chosen as the searched words/ quotes and retweets eliminated and language is selected as eng\n",
    "    query = \" (a OR the OR i OR you) lang:en -is:quote -is:retweet -is:reply\"\n",
    "\n",
    "    print(f\"Searching for \\\"{query}\\\" tweets from {start_time} to {end_time}...\")\n",
    "\n",
    "    # search_results is a generator, max_results is max tweets per page, not total, 100 is max when using all expansions.\n",
    "    search_results = t.search_recent(query=query, start_time=start_time, end_time=end_time, max_results=100)\n",
    "\n",
    "    # Get all results page by page:\n",
    "    i = 0\n",
    "    for page in search_results:\n",
    "        while i < 6: \n",
    "        # Converting a python object into an equivalent JSON object:\n",
    "            with open(\"XXX\" + str(y) + \".jsonl\", \"w+\") as f:\n",
    "                f.write(json.dumps(page) + \"\\n\")\n",
    "            print(\"Wrote a page of results...\")\n",
    "            i += 1\n",
    "        break\n",
    "\n",
    "    print(\"Converting to CSV...\")\n",
    "\n",
    "\n",
    "    # Converting a JSON object into a CSV.\n",
    "    with open(\"XXX\" + str(y) + \".jsonl\", \"r\") as infile:\n",
    "        with open(\"XXX\" + str(y) + \".csv\", \"w\") as outfile:\n",
    "            converter = CSVConverter(infile, outfile)\n",
    "            converter.process()\n",
    "            \n",
    "    y = y + 1\n",
    "\n",
    "    print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d19322",
   "metadata": {},
   "source": [
    "# CSV Merging and Creating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f95760",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'XXX' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0, encoding='utf-8')\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, df.columns.isin([\"id\",\"conversation_id\",\"author_id\",\n",
    "                           \"created_at\",\"text\",\"lang\",\"source\",\"public_metrics.like_count\",\n",
    "                           \"public_metrics.quote_count\",\"public_metrics.reply_count\",\"public_metrics.retweet_count\",\n",
    "                           \"reply_settings\",\"possibly_sensitive\",\"entities.annotations\",\"entities.cashtags\",\"entities.hashtags\",\n",
    "                           \"entities.mentions\",\"entities.urls\",\"context_annotations\",\"attachments.media\",\"attachments.poll.id\",\n",
    "                           \"author.id\",\"author.created_at\",\"author.public_metrics.followers_count\",\n",
    "                           \"author.public_metrics.listed_count\",\"author.public_metrics.tweet_count\",\"author.verified\",\n",
    "                           \"geo.country\",\"geo.country_code\",\"__twarc.retrieved_at\",\"__twarc.url\",\"__twarc.version\"])]\n",
    "                                  \n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bff300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any duplicates by id column\n",
    "duplicateRowsDF = df[df.duplicated(['id'])]\n",
    "print(\"Duplicate Rows based on a single column are:\", duplicateRowsDF, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['geo.country_code'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_diff\"] = pd.to_datetime(df[\"__twarc.retrieved_at\"]) - pd.to_datetime(df[\"created_at\"])\n",
    "\n",
    "df[\"time_diff_hours\"] = ((pd.to_datetime(df[\"__twarc.retrieved_at\"]) - pd.to_datetime(df[\"created_at\"])).dt.total_seconds())/3600\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d50a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the diff between twarc extraction time and tweet creation time should be longer than 24 hours\n",
    "df = df[df['time_diff_hours'] >= 24]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3690a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot frequency of users, sources, etc.\n",
    "def plot_frequency_charts(df, feature, title):\n",
    "    freq_df = pd.DataFrame()\n",
    "    freq_df[feature] = df[feature]\n",
    "    \n",
    "    f, ax = plt.subplots(1,1, figsize=(16,4))\n",
    "    total = float(len(df))\n",
    "    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n",
    "    g.set_title(\"Number and percentage of {}\".format(title))\n",
    "\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(100*height/total),\n",
    "                ha=\"center\") \n",
    "\n",
    "    plt.title('Frequency of {} '.format(feature))\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.xlabel(title, fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency_charts(df, 'author.verified','author.verified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg of engagement metrics depending on verification of author\n",
    "df.groupby(\"author.verified\", as_index=True)[['public_metrics.like_count', 'public_metrics.quote_count', 'public_metrics.reply_count', \n",
    "                                              'public_metrics.retweet_count']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The account shouldn't represent or associated with a prominently recognized individual or brand\n",
    "df = df[df['author.verified'] != 1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"reply_settings\", as_index=True)[['public_metrics.reply_count']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The account shouldn't represent or associated with a prominently recognized individual or brand\n",
    "df = df[df['reply_settings'] == 'everyone'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disregard authors that have more than 10k followers\n",
    "df = df[df['author.public_metrics.followers_count'] <= 10000]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b00d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the missing values\n",
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fa55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking the follower count of authers by filtering the data via iqr\n",
    "Q1 = df['author.public_metrics.followers_count'].quantile(0.25)\n",
    "Q3 = df['author.public_metrics.followers_count'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "\n",
    "filter = (df['author.public_metrics.followers_count'] >= Q1 - 1.5 * IQR) & (df['author.public_metrics.followers_count'] <= Q3 + 1.5 *IQR)\n",
    "sns.boxplot(data=df, x=df['author.public_metrics.followers_count'].loc[filter], sym=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75802f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a total engagement column\n",
    "column_names = ['public_metrics.like_count', 'public_metrics.retweet_count', 'public_metrics.reply_count']\n",
    "df['Engagement_Total']= df[column_names].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking the follower total engagement by filtering the data via iqr\n",
    "Q1 = df['Engagement_Total'].quantile(0.25)\n",
    "Q3 = df['Engagement_Total'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "\n",
    "\n",
    "filter = (df['Engagement_Total'] >= Q1 - 1.5 * IQR) & (df['Engagement_Total'] <= Q3 + 1.5 *IQR)\n",
    "sns.boxplot(data=df, x=df['Engagement_Total'].loc[filter], sym=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ad643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing flesch scores\n",
    "filter = (df['flesch_reading'] >= -200)\n",
    "sns.displot(data=df, x=df['flesch_reading'].loc[filter], kind=\"kde\", bw_adjust=.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77627ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examining why some texts are too complex\n",
    "df.sort_values(by='flesch_reading', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711b6e6",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e038e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing ekphrasis\n",
    "#!pip install ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382939e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a65075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    #annotate={\"hashtag\", \"elongated\", \"repeated\",\n",
    "    #    'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words /“ahhhh.” “hmmmm”\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "    text = text.replace(\"“\", \"'\")\n",
    "    text = text.replace(\"”\", \"'\")\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is \", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"it's\", \" it is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \"  are\", text)\n",
    "    text = re.sub(r\"\\'d\", \"  would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    \n",
    "    #handling \\n\n",
    "    text = text.replace(\"\\\\n\", \" \")   \n",
    "    \n",
    "    # remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text) \n",
    "\n",
    "\n",
    "    return text\n",
    "df['text_preprocessed'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    text = \" \".join(text_processor.pre_process_doc(text))\n",
    "    text = re.sub(r'[%s]' % re.escape(''.join(string.punctuation)), r' ',text)\n",
    "    return text\n",
    "\n",
    "df['text_preprocessed'] = df['text_preprocessed'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b426883",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text','text_preprocessed']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3395c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#average length of words in tweet\n",
    "df['avg_length_words'] = df[\"text_preprocessed\"].apply(lambda x: np.mean([len(w) for w in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45dbdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emoji and word count\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_counter = 0\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emoji_counter += 1\n",
    "            # Remove from the given text the emojis\n",
    "            text = text.replace(word, '') \n",
    "\n",
    "    words_counter = len(text.split())\n",
    "\n",
    "    return emoji_counter\n",
    "df['emoji_count'] = df['text'].apply(split_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe416158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_preprocessed_nostopwords\"] = df[\"text_preprocessed\"].apply(lambda text: remove_stopwords(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5153ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column for created hour \n",
    "import dateutil.parser as p\n",
    "\n",
    "def to_datetime(datestring):\n",
    "    x = p.parse(datestring)\n",
    "    return x.strftime(\"%H:%M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at_hour'] = df['created_at'].apply(to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573960ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there is hashtags, mentions, urls, media, poll\n",
    "def check_entities(x):\n",
    "    if x != x:\n",
    "        x = 0\n",
    "    else:\n",
    "        x = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['check_hashtags'] = df['entities.hashtags'].apply(check_entities)\n",
    "df['check_mentions'] = df['entities.mentions'].apply(check_entities)\n",
    "df['check_urls'] = df['entities.urls'].apply(check_entities)\n",
    "df['check_media'] = df['attachments.media'].apply(check_entities)\n",
    "df['check_poll'] = df['attachments.poll.id'].apply(check_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d892e8",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11aaf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df[[\"public_metrics.like_count\", \"public_metrics.quote_count\", \n",
    "          \"public_metrics.reply_count\", \"public_metrics.retweet_count\", \n",
    "          \"possibly_sensitive\", \"author.public_metrics.followers_count\", \n",
    "          \"author.public_metrics.listed_count\",\n",
    "          \"author.public_metrics.tweet_count\", \n",
    "          \"emoji_count\", \"check_hashtags\",\"mention_count\",\"url_count\",\"check_media\",\n",
    "          \"check_poll\",\"flesch_reading\", \"avg_length_words\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbf421",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "corr = df5.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(df5.corr(), \n",
    "            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot=True, annot_kws={\"size\": 8}, square=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f794b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports PIL module \n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud,ImageColorGenerator,STOPWORDS\n",
    "\n",
    "\n",
    "\n",
    "def green_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
    "    color = '#00ff00'    \n",
    "    return color\n",
    "\n",
    "#restricted = ['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "#        'time', 'date', 'number'] \n",
    "\n",
    "#df_restricted = df[~df.text_preprocessed_nostopwords.isin(restricted)]\n",
    "\n",
    "stop_words = ['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number']  + list(STOPWORDS)\n",
    "\n",
    "logo = np.array(Image.open('XXX'))\n",
    "\n",
    "dis_wc = WordCloud(stopwords=stop_words,\n",
    "            collocations=False,\n",
    "            background_color=\"Black\",mask=logo).generate(' '.join(df['text_preprocessed_nostopwords']))\n",
    "image_colors = ImageColorGenerator(logo)\n",
    "dis_wc.recolor(color_func=green_color_func, random_state=3)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(20,6))\n",
    "\n",
    "ax1.imshow(dis_wc)\n",
    "ax1.set_title(\"Word cloud for tweets\", fontsize=20)\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996db94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the most common words\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"text\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d760e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the most common words after removing stopwords\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"text_preprocessed_nostopwords\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['author.public_metrics.followers_count']>0) & (df['author.public_metrics.tweet_count']>1)][['public_metrics.like_count', 'public_metrics.reply_count', 'public_metrics.retweet_count', 'Engagement_Total']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac50e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['author.public_metrics.followers_count']>0) & (df['author.public_metrics.tweet_count']>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['public_metrics.like_count', 'public_metrics.reply_count', 'public_metrics.retweet_count', 'Engagement_Total']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45dcb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = df[['public_metrics.like_count','public_metrics.quote_count', 'public_metrics.reply_count', 'public_metrics.retweet_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_columns = combined_df.columns\n",
    "num_col = combined_df._get_numeric_data().columns\n",
    "cat_col = list(set(total_columns)-set(num_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d231326",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_num_df = combined_df.describe(include=[\"int64\",\"float64\"])\n",
    "describe_num_df.reset_index(inplace=True)\n",
    "\n",
    "describe_num_df = describe_num_df[describe_num_df[\"index\"] != \"count\"]\n",
    "for i in num_col:\n",
    "    if i in [\"index\"]:\n",
    "        continue\n",
    "    sns.factorplot(x=\"index\", y=i, data=describe_num_df)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "t = [2,4]\n",
    "df.loc[~df.index.isin(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(datestring):\n",
    "    x = p.parse(datestring)\n",
    "    return x.strftime(\"%H:%M\")\n",
    "def to_date(datestring):\n",
    "    x = p.parse(datestring)\n",
    "    return x.strftime('%Y-%m-%d')\n",
    "\n",
    "from datetime import datetime\n",
    "#df_new = df\n",
    "#split the time column into hour and date\n",
    "#df_new['date'] = df_new['created_at']\n",
    "df['date_1'] = df['created_at'].apply(to_date)\n",
    "df['hour'] = df['created_at'].apply(to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add weekday columns \n",
    "df['day'] = pd.to_datetime(df.date_1, format='%Y-%m-%d %H:%M:%S').dt.weekday\n",
    "df['day_name'] = pd.to_datetime(df.date_1, format='%Y-%m-%d %H:%M:%S').dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650fbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add weekday columns \n",
    "df_new['day'] = df_new['date_1'].dt.weekday\n",
    "df_new['day_name'] = df_new['date_1'].dt.day_name()\n",
    "\n",
    "#pick only relevant columns \n",
    "week_df = df_new[['public_metrics.like_count','public_metrics.quote_count', 'public_metrics.reply_count', 'public_metrics.retweet_count',\n",
    "                  'emoji_count', 'day','day_name', 'avg_length_words']]\n",
    "\n",
    "# create a group by object\n",
    "weekday_grouped = df_new.groupby('day_name')\n",
    "\n",
    "#create a df with means sorted by day\n",
    "week_mean = weekday_grouped.mean().sort_values(by=\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_means_by_weekday(dataframe, variable_1, variable_2=None, variable_3=None, variable_4=None):\n",
    "    #create a figure\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "    # set x and y axes\n",
    "    x = dataframe.index\n",
    "    y1 = dataframe[variable_1]\n",
    "    #plot\n",
    "    ax.plot(x, y1, label=variable_1)\n",
    "    \n",
    "    #add other plots if variables given\n",
    "    if variable_2 != None:\n",
    "        y2 = dataframe[variable_2]\n",
    "        ax.plot(x, y2, label=variable_2)\n",
    "        \n",
    "    if variable_3 != None:\n",
    "        y3 = dataframe[variable_3]\n",
    "        ax.plot(x, y3, label=variable_3)\n",
    "        \n",
    "    if variable_4 != None:\n",
    "        y4 = dataframe[variable_4]\n",
    "        ax.plot(x, y4, label=variable_4)\n",
    "\n",
    "    #format title and labels\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Average engagement by the day of the week\", fontsize=14)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_means_by_weekday(week_mean, 'public_metrics.retweet_count',\n",
    "                      'public_metrics.like_count',\n",
    "                      'public_metrics.quote_count','public_metrics.reply_count')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc561f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a group by object\n",
    "weekday_grouped_1 = df_new.groupby(['day_name','day'])\n",
    "\n",
    "#plot number of tweets and mean impressions\n",
    "weekday_count = weekday_grouped_1.agg(['count','mean'])\n",
    "\n",
    "#pick the impressions column and turn into a separate df\n",
    "week_impressions = weekday_count['public_metrics.like_count']\n",
    "#set index to be the day\n",
    "week_impressions = week_impressions.reset_index()\n",
    "week_impressions = week_impressions.set_index('day_name')\n",
    "#sort day names\n",
    "week_impressions = week_impressions.sort_values(by='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5cfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot correlations\n",
    "fig, ax = plt.subplots(figsize=(15,15)) \n",
    "_ = sns.heatmap(corr, annot = True, ax=ax)\n",
    "plt.title(\"Correlation matrix of engagement metrics\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eec710",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['Engagement_Total'].quantile(0.25)\n",
    "Q3 = df['Engagement_Total'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "\n",
    "filter = (df['Engagement_Total'] >= Q1 - 1.5 * IQR) & (df['Engagement_Total'] <= Q3 + 1.5 *IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x=df['Engagement_Total'].loc[filter], sym=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e488ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(df['reply_settings'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = df\n",
    "\n",
    "# Texts range from 10 to 350 characters and generally, it is between 40 to 150 characters.\n",
    "df_eda['text'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a78673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting url and mentions\n",
    "\n",
    "words = [\"url\"]\n",
    "df[\"url_count\"] = df.text_preprocessed.apply(lambda x: sum([x.count(word) for word in words]))\n",
    "\n",
    "words2 = [\"user\"]\n",
    "df[\"mention_count\"] = df.text_preprocessed.apply(lambda x: sum([x.count(word) for word in words2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297b88f",
   "metadata": {},
   "source": [
    "# Emotion and Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58380219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='emotion'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dcef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translationPipeline(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    #for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[0]]\n",
    "    s = scores[ranking[0]]\n",
    "    output = f\" {l} {np.round(float(s), 4)}\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d966cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_preprocessed_emotion_scores'] = df['text_preprocessed'].apply(translationPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8688b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translationPipeline(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    #for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[0]]\n",
    "    s = scores[ranking[0]]\n",
    "    output = f\" {l}\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_preprocessed_sentiment'] = df['text_preprocessed'].apply(translationPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c633d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('XXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency_charts(df, 'text_preprocessed_sentiment','text_preprocessed_sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e1313",
   "metadata": {},
   "source": [
    "# Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aaed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7333c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flesch Reading Ease Score. While the maximum score is 121.22, there is no limit on how low the score can be. A negative score is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readingscore(text):\n",
    "\n",
    "    score = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    return score\n",
    "\n",
    "df['flesch_reading'] = df['text_preprocessed_nostopwords'].apply(readingscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaee44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_preprocessed_emotion_heading\"] = np.nan\n",
    "for i in range(df.shape[0]):\n",
    "    if df['text_preprocessed_emotion_scores'].astype(str).str.split().str[1].astype(float)[i] < 0.5:\n",
    "        df['text_preprocessed_emotion_heading'][i] = 'Unknown'\n",
    "        print(i)\n",
    "        print(df['text_preprocessed_emotion_heading'][i])\n",
    "    else:\n",
    "        df['text_preprocessed_emotion_heading'][i] = df['text_preprocessed_emotion_scores'].astype(str).str.split().str[0][i]\n",
    "        print(i)\n",
    "        print(df['text_preprocessed_emotion_heading'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
